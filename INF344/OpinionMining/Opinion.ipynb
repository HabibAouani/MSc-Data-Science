{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Analyse des opinions sous twitter\n",
    "\n",
    "Maël Fabien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:50:59.861946Z",
     "start_time": "2019-06-17T14:50:59.848216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/maelfabien/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### General import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "### NLTK\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk as nltk\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Importer les fichiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les tweets à analyser sont disponibles à l’adresse suivante : https://clavel.wp.imt.fr/files/ 2018/05/testdata.manual.2009.06.14.csv_.zip. Cette base (Sentiment140) a été obtenue sur le site de l’université de Stanford http://help.sentiment140.com/for-students. Un extrait en est donné dans le tableau 1. La base contient 498 tweets annotés manuellement. La base propose 6 champs corres- pondant aux informations suivantes :\n",
    "1. la polarité du tweet : Chaque tweet est accompagné d’un score pouvant être égal à 0 (négatif), 2 (neutre) ou 4 (positif).\n",
    "2. l’identifiant du tweet (2087)\n",
    "3. la date du tweet (Sat May 16 23 :58 :44 UTC 2009)\n",
    "4. la requête associée (lyx). Si pas de requête la valeur est NO_ QUERY.\n",
    "5. l’utilisateur qui a tweeté (robotickilldozr)\n",
    "6. le texte du tweet(Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:00.362327Z",
     "start_time": "2019-06-17T14:51:00.358977Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:00.549278Z",
     "start_time": "2019-06-17T14:51:00.517429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:22:00 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>GeorgeVHulme</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 11 03:22:30 UTC 2009</td>\n",
       "      <td>aig</td>\n",
       "      <td>Seth937</td>\n",
       "      <td>Fuck this economy. I hate aig and their non lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:26:10 UTC 2009</td>\n",
       "      <td>jquery</td>\n",
       "      <td>dcostalis</td>\n",
       "      <td>Jquery is my new best friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:27:15 UTC 2009</td>\n",
       "      <td>twitter</td>\n",
       "      <td>PJ_King</td>\n",
       "      <td>Loves twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:29:20 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>mandanicole</td>\n",
       "      <td>how can you not love Obama? he makes jokes abo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Mon May 11 03:32:42 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>jpeb</td>\n",
       "      <td>Check this video out -- President Obama at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 11 03:32:48 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>kylesellers</td>\n",
       "      <td>@Karoli I firmly believe that Obama/Pelosi hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:33:38 UTC 2009</td>\n",
       "      <td>obama</td>\n",
       "      <td>theviewfans</td>\n",
       "      <td>House Correspondents dinner was last night who...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 05:05:58 UTC 2009</td>\n",
       "      <td>nike</td>\n",
       "      <td>MumsFP</td>\n",
       "      <td>Watchin Espn..Jus seen this new Nike Commerica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>Mon May 11 05:06:22 UTC 2009</td>\n",
       "      <td>nike</td>\n",
       "      <td>vincentx24x</td>\n",
       "      <td>dear nike, stop with the flywire. that shit is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                             2        3             4  \\\n",
       "0   4  Mon May 11 03:17:40 UTC 2009  kindle2        tpryan   \n",
       "1   4  Mon May 11 03:18:03 UTC 2009  kindle2        vcu451   \n",
       "2   4  Mon May 11 03:18:54 UTC 2009  kindle2        chadfu   \n",
       "3   4  Mon May 11 03:19:04 UTC 2009  kindle2         SIX15   \n",
       "4   4  Mon May 11 03:21:41 UTC 2009  kindle2      yamarama   \n",
       "5   4  Mon May 11 03:22:00 UTC 2009  kindle2  GeorgeVHulme   \n",
       "6   0  Mon May 11 03:22:30 UTC 2009      aig       Seth937   \n",
       "7   4  Mon May 11 03:26:10 UTC 2009   jquery     dcostalis   \n",
       "8   4  Mon May 11 03:27:15 UTC 2009  twitter       PJ_King   \n",
       "9   4  Mon May 11 03:29:20 UTC 2009    obama   mandanicole   \n",
       "10  2  Mon May 11 03:32:42 UTC 2009    obama          jpeb   \n",
       "11  0  Mon May 11 03:32:48 UTC 2009    obama   kylesellers   \n",
       "12  4  Mon May 11 03:33:38 UTC 2009    obama   theviewfans   \n",
       "13  4  Mon May 11 05:05:58 UTC 2009     nike        MumsFP   \n",
       "14  0  Mon May 11 05:06:22 UTC 2009     nike   vincentx24x   \n",
       "\n",
       "                                                    5  \n",
       "0   @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1   Reading my kindle2...  Love it... Lee childs i...  \n",
       "2   Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3   @kenburbary You'll love your Kindle2. I've had...  \n",
       "4   @mikefish  Fair enough. But i have the Kindle2...  \n",
       "5   @richardebaker no. it is too big. I'm quite ha...  \n",
       "6   Fuck this economy. I hate aig and their non lo...  \n",
       "7                       Jquery is my new best friend.  \n",
       "8                                       Loves twitter  \n",
       "9   how can you not love Obama? he makes jokes abo...  \n",
       "10  Check this video out -- President Obama at the...  \n",
       "11  @Karoli I firmly believe that Obama/Pelosi hav...  \n",
       "12  House Correspondents dinner was last night who...  \n",
       "13  Watchin Espn..Jus seen this new Nike Commerica...  \n",
       "14  dear nike, stop with the flywire. that shit is...  "
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('testdata.manual.2009.06.14.csv', header=None)\n",
    "df = df.drop([1], axis=1)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:00.751264Z",
     "start_time": "2019-06-17T14:51:00.745722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute sentiment\n",
    "score = {}\n",
    "score[0] = 'Negatif'\n",
    "score[2] = 'Neutre'\n",
    "score[4] = 'Positif'\n",
    "\n",
    "df['sent'] = df[0].apply(lambda x : score[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Prétraitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les tweets contiennent des caractères spéciaux susceptibles de nuire à la mise en place des méthodes d’analyse d’opinions. Ecrire un programme permettant pour chaque tweet de :\n",
    "\n",
    "- récupérer le texte associé\n",
    "- segmenter en tokens\n",
    "- supprimer les urls\n",
    "- nettoyer les caractères inhérents à la structure d’un tweet\n",
    "- corriger les abréviations et les spécificités langagières des tweets à l’aide du dictionnaire DicoS- lang (fichier SlangLookupTable.txt disponible ici : https://clavel.wp.imt.fr/files/2018/06/ Lexiques.zip), encodage du fichier : latin1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:01.218263Z",
     "start_time": "2019-06-17T14:51:01.204052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121</td>\n",
       "      <td>one to one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a/s/l</td>\n",
       "      <td>age, sex, location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adn</td>\n",
       "      <td>any day now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afaik</td>\n",
       "      <td>as far as I know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afk</td>\n",
       "      <td>away from keyboard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                   1\n",
       "0    121          one to one\n",
       "1  a/s/l  age, sex, location\n",
       "2    adn         any day now\n",
       "3  afaik    as far as I know\n",
       "4    afk  away from keyboard"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slang lookup table\n",
    "slang = pd.read_csv('Lexiques/SlangLookupTable.txt', encoding='latin1', sep='\\t', header=None)\n",
    "slang.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous préciserez dans le CR le nombre d’occurrences des caractères inhérents à la struc- ture du tweet et le nombre d’occurrences des ’hash-tags’ dans le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:01.538235Z",
     "start_time": "2019-06-17T14:51:01.526855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count characters\n",
    "def count_char(txt) :\n",
    "    \n",
    "    count_hastag = 0\n",
    "    count_struc = 0\n",
    "    \n",
    "    for string in txt :\n",
    "        count_hastag += string.count(\"#\")\n",
    "        count_struc += string.count(\"@\")\n",
    "        \n",
    "    return \"There are %d hashtags and %d tweet inherecent characters.\" % (count_hastag, count_struc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:01.720136Z",
     "start_time": "2019-06-17T14:51:01.709618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 52 hashtags and 128 tweet inherecent characters.'"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_char(list(df[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:01.974313Z",
     "start_time": "2019-06-17T14:51:01.961540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean the text\n",
    "\n",
    "def treat_text(txt) :\n",
    "    \n",
    "    # Remove URLs\n",
    "    txt = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', txt, flags=re.MULTILINE)\n",
    "    \n",
    "    # Replace characters\n",
    "    txt = txt.replace(\"@\", \"\")\n",
    "    txt = txt.replace(\"#\", \"\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    \n",
    "    # Replace slang language\n",
    "    for i in range(len(tokens)) :\n",
    "        if tokens[i] in slang[0] :\n",
    "            tokens[i] = slang[slang[0] == tokens[i]][1]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:02.186216Z",
     "start_time": "2019-06-17T14:51:02.069650Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = df[5].apply(lambda x : treat_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:02.259668Z",
     "start_time": "2019-06-17T14:51:02.248649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [stellargirl, I, looovvveee, my, Kindle, 2, .,...\n",
       "1     [Reading, my, kindle, 2, ..., Love, it, ..., L...\n",
       "2     [Ok, ,, first, assesment, of, the, kindle, 2, ...\n",
       "3     [kenburbary, You'll, love, your, Kindle, 2, .,...\n",
       "4     [mikefish, Fair, enough, ., But, i, have, the,...\n",
       "5     [richardebaker, no, ., it, is, too, big, ., I'...\n",
       "6     [Fuck, this, economy, ., I, hate, aig, and, th...\n",
       "7                [Jquery, is, my, new, best, friend, .]\n",
       "8                                      [Loves, twitter]\n",
       "9     [how, can, you, not, love, Obama, ?, he, makes...\n",
       "10    [Check, this, video, out, -, -, President, Oba...\n",
       "11    [Karoli, I, firmly, believe, that, Obama, /, P...\n",
       "12    [House, Correspondents, dinner, was, last, nig...\n",
       "13    [Watchin, Espn, .., Jus, seen, this, new, Nike...\n",
       "14    [dear, nike, ,, stop, with, the, flywire, ., t...\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Etiquetage Grammatical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Développer une fonction capable de déterminer la catégorie grammaticale (POS : Part Of Speech) de chaque mot du tweet en utilisant la commande suivante de la libraire nltk :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:03.007852Z",
     "start_time": "2019-06-17T14:51:02.998728Z"
    }
   },
   "outputs": [],
   "source": [
    "def POS(tokens) :\n",
    "    \n",
    "    list_pos = []\n",
    "    \n",
    "    # For each token\n",
    "    for token in tokens:\n",
    "        \n",
    "        # Append the POS tag\n",
    "        list_pos.append(nltk.pos_tag(token))\n",
    "        \n",
    "    return list_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:03.907865Z",
     "start_time": "2019-06-17T14:51:03.279443Z"
    }
   },
   "outputs": [],
   "source": [
    "taggedData = POS(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Algorithme de détection - V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK dispose entre autre d’une interface pour manipuler la base de données WordNet. Ainsi, après installation de NLTK et du package WordNet, un utilisateur peut accéder à l’ensemble des synsets qui sont liés à un mot donné à l’aide d’une commande simple sous Python. Observez son fonctionnement à l’aide des lignes de code suivantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:05.174095Z",
     "start_time": "2019-06-17T14:51:05.164569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette étape, vous devez développer un programme permettant :\n",
    "- de récupérer uniquement les mots correspondant à des adjectifs, noms, adverbes et verbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:06.401420Z",
     "start_time": "2019-06-17T14:51:06.394420Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_words(taggedData) :\n",
    "    list_token = []\n",
    "    \n",
    "    # For each tweet\n",
    "    for tokens in taggedData :\n",
    "        intermediate_list_token = []\n",
    "        # For each token\n",
    "        for token in tokens : \n",
    "            # Keep only tokens whose POS starts with the following letters\n",
    "            if token[1][:2] == 'JJ' or token[1][:2] == 'NN' or token[1][:2] == 'VB' or token[1][:2] == 'JJ' == 'RB' :\n",
    "                intermediate_list_token.append(token[0])\n",
    "        list_token.append(intermediate_list_token)\n",
    "    # Returns a list of lists\n",
    "    return list_token\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:07.162668Z",
     "start_time": "2019-06-17T14:51:07.153585Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered_words = sort_words(taggedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d’accéder aux scores (positifs et négatifs) des synsets dans la librairie NLTK. Ce script définira dans une classe Python l’objet SentiSynset sur le même modèle que le Synset développé dans NLTK pour WordNet, et permettra de lire le tableau de SentiWordNet comme suit.\n",
    "- de calculer pour chaque mot les scores associés à leur premier synset,\n",
    "- de calculer pour chaque tweet la somme des scores positifs et négatifs des SentiSynsets du tweet,\n",
    "- de comparez la somme des scores positifs et des scores négatifs de chaque tweet pour décider de la classe à associer au tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:08.247179Z",
     "start_time": "2019-06-17T14:51:08.222820Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_score_v1(taggedData) :\n",
    "    \n",
    "    # Move the function defined above here\n",
    "    def sort_words(taggedData) :\n",
    "        list_token = []\n",
    "        for tokens in taggedData :\n",
    "            intermediate_list_token = []\n",
    "            for token in tokens :\n",
    "                if token[1][:2] == 'JJ' or token[1][:2] == 'NN' or token[1][:2] == 'VB' or token[1][:2] == 'JJ' == 'RB' :\n",
    "                    intermediate_list_token.append(token[0])\n",
    "            list_token.append(intermediate_list_token)\n",
    "        return list_token\n",
    "\n",
    "    filtered_words = sort_words(taggedData)\n",
    "    score_tweet = []\n",
    "        \n",
    "    # For each tweer\n",
    "    for tweet in filtered_words :\n",
    "        \n",
    "        # Initalize the scores\n",
    "        score = 0\n",
    "        score_pos = 0\n",
    "        score_neg = 0\n",
    "        \n",
    "        # For each token within the tweer\n",
    "        for token in tweet :\n",
    "            \n",
    "            try :\n",
    "                # Try to compute and add the positive and negative scores linked to the first wordnet\n",
    "                score_pos += swn.senti_synset(wn.synsets(token)[0].name()).pos_score()\n",
    "                score_neg += swn.senti_synset(wn.synsets(token)[0].name()).neg_score()\n",
    "                    \n",
    "            except : \n",
    "                pass\n",
    "        \n",
    "        # Format of the output : score pos, score neg, label\n",
    "        if score_pos > score_neg :\n",
    "            score_tweet.append([score_pos, score_neg, 'Positif'])\n",
    "        elif score_pos == score_neg :\n",
    "            score_tweet.append([score_pos, score_neg, 'Neutre'])\n",
    "        else : \n",
    "            score_tweet.append([score_pos, score_neg, 'Negatif'])\n",
    "            \n",
    "    return np.array(score_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:09.440919Z",
     "start_time": "2019-06-17T14:51:09.089341Z"
    }
   },
   "outputs": [],
   "source": [
    "score_v1 = compute_score_v1(taggedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:09.452183Z",
     "start_time": "2019-06-17T14:51:09.444838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5341365461847389"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df['sent'], score_v1[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:09.674599Z",
     "start_time": "2019-06-17T14:51:09.665928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['sent'] == score_v1[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuracy atteint 53.4% et on identifie correctement 266 labels sur les 498."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de détection - V2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous aurez besoin de : \n",
    "- la liste des mots en anglais correspondant à des négations (fichier NegatingWordList.txt disponible ici : https://clavel.wp.imt.fr/files/2018/06/Lexiques.zip) \n",
    "- et celle correspondant aux modifieurs (fichier BoosterWordList.txt disponible ici : https://clavel.wp.imt.fr/files/2018/06/Lexiques.zip). \n",
    "\n",
    "Pour chaque mot, l’algorithme doit effectuer les opérations suivantes :\n",
    "- multiplie par 2 le score négatif et le score positif associés au mot si le mot précédent est un modifieur ;\n",
    "- utilise uniquement le score négatif du mot pour le score positif global du tweet et le score positif du mot pour le score négatif global du tweet si le mot précédent est une négation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:12.329546Z",
     "start_time": "2019-06-17T14:51:12.323289Z"
    }
   },
   "outputs": [],
   "source": [
    "negating = [\"aren't\",\"arent\", \"can't\", \"cannot\", \"cant\", \"don't\", \"dont\", \"isn't\", \"isnt\", \"never\", \"not\", \"won't\", \"wont\", \"wouldn't\", \"wouldnt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:51:12.658265Z",
     "start_time": "2019-06-17T14:51:12.640932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>absolutely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>definitely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>extremely</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fuckin</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fucking</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hugely</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>incredibly</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>just</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>overwhelmingly</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>so</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>some</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sum</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>very</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1\n",
       "0       absolutely  1\n",
       "1       definitely  1\n",
       "2        extremely  2\n",
       "3           fuckin  2\n",
       "4          fucking  2\n",
       "5           hugely  2\n",
       "6       incredibly  2\n",
       "7             just -1\n",
       "8   overwhelmingly  2\n",
       "9               so  0\n",
       "10            some -1\n",
       "11             sum -1\n",
       "12            very  1"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster = pd.read_csv('Lexiques/BoosterWordList.txt', header=None, delimiter='\\t')\n",
    "booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:55:16.175652Z",
     "start_time": "2019-06-17T14:55:16.091931Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_score_v2(taggedData) :\n",
    "    \n",
    "    def sort_words(taggedData) :\n",
    "        list_token = []\n",
    "        for tokens in taggedData :\n",
    "            intermediate_list_token = []\n",
    "            for token in tokens :\n",
    "                if token[1][:2] == 'JJ' or token[1][:2] == 'NN' or token[1][:2] == 'VB' or token[1][:2] == 'JJ' == 'RB' :\n",
    "                    intermediate_list_token.append(token[0])\n",
    "            list_token.append(intermediate_list_token)\n",
    "        return list_token\n",
    "\n",
    "    filtered_words = sort_words(taggedData)\n",
    "    score_tweet = []\n",
    "    \n",
    "    for tweet in filtered_words :\n",
    "        modifier = None\n",
    "        negator = None\n",
    "        \n",
    "        score = 0\n",
    "        score_pos = 0\n",
    "        score_neg = 0\n",
    "        \n",
    "        total_negating = 0\n",
    "        pos_negating = 0\n",
    "    \n",
    "        for token in tweet :\n",
    "            \n",
    "            try :\n",
    "                # Check if the previous word in negating list (and inverse scores)\n",
    "                if negator in negating :\n",
    "                    # Total negating words\n",
    "                    total_negating += 1\n",
    "                    # Check if previous word in booster list (and double score)\n",
    "                    if modifier in booster[0] :\n",
    "                        score_pos += swn.senti_synset(wn.synsets(token)[0].name()).neg_score() * 2\n",
    "                        score_neg += swn.senti_synset(wn.synsets(token)[0].name()).pos_score() * 2\n",
    "                    else :\n",
    "                        score_pos += swn.senti_synset(wn.synsets(token)[0].name()).neg_score()\n",
    "                        score_neg += swn.senti_synset(wn.synsets(token)[0].name()).pos_score()\n",
    "                else :\n",
    "                    if modifier in booster[0] :\n",
    "                        score_pos += swn.senti_synset(wn.synsets(token)[0].name()).pos_score() * 2\n",
    "                        score_neg += swn.senti_synset(wn.synsets(token)[0].name()).neg_score() * 2\n",
    "                    else :\n",
    "                        score_pos += swn.senti_synset(wn.synsets(token)[0].name()).pos_score()\n",
    "                        score_neg += swn.senti_synset(wn.synsets(token)[0].name()).neg_score()\n",
    "                    \n",
    "            except : \n",
    "                pass\n",
    "            \n",
    "            # Set the current token as the modifier and the negator for the next iteration\n",
    "            modifier = token\n",
    "            negator = token\n",
    "            \n",
    "        \n",
    "        if score_pos > score_neg :\n",
    "            # Count number of positive tweets with negating words\n",
    "            if total_negating > 0 :\n",
    "                pos_negating =+ 1\n",
    "            score_tweet.append([score_pos, score_neg, 'Positif'])\n",
    "        elif score_pos == score_neg :\n",
    "            score_tweet.append([score_pos, score_neg, 'Neutre'])\n",
    "        else : \n",
    "            score_tweet.append([score_pos, score_neg, 'Negatif'])\n",
    "    print(\"Total number of negating terms in positive tweets : \" + str(pos_negating))\n",
    "    return np.array(score_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:55:16.879098Z",
     "start_time": "2019-06-17T14:55:16.533858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of negating terms in positive tweets : 0\n"
     ]
    }
   ],
   "source": [
    "score_v2 = compute_score_v2(taggedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:55:17.797265Z",
     "start_time": "2019-06-17T14:55:17.789400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5381526104417671"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df['sent'], score_v2[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:55:18.575974Z",
     "start_time": "2019-06-17T14:55:18.564917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['sent'] == score_v2[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuracy augmente légèrement avec cette nouvelle version, et on classifie correctement 2 exemples de plus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de détection - V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous avez ici besoin du dictionnaire d’émoticons est disponible (fichier EmoticonLookupTable.txt disponible ici : https://clavel.wp.imt.fr/files/2018/06/Lexiques.zip). \n",
    "\n",
    "Cet algorithme demande en entrée deux listes supplémentaires : \n",
    "- une liste d’emoticons positifs \n",
    "- et une liste d’émoticons négatifs\n",
    "\n",
    "Les émoticons positifs rencontrés augmentent de 1 la valeur du score positif du tweet, tandis que les émoticons négatifs augmentent de 1 la valeur du score négatif du tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:55:24.819167Z",
     "start_time": "2019-06-17T14:55:24.809203Z"
    }
   },
   "outputs": [],
   "source": [
    "emo = pd.read_csv('Lexiques/EmoticonLookupTable.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T14:55:25.177277Z",
     "start_time": "2019-06-17T14:55:25.169313Z"
    }
   },
   "outputs": [],
   "source": [
    "emo_pos = list(emo[emo[1] > 0][0])\n",
    "emo_neg = list(emo[emo[1] < 0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:00:07.904774Z",
     "start_time": "2019-06-17T15:00:07.849147Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_score_v3(taggedData) :\n",
    "    \n",
    "    def sort_words(taggedData) :\n",
    "        list_token = []\n",
    "        for tokens in taggedData :\n",
    "            intermediate_list_token = []\n",
    "            for token in tokens :\n",
    "                if token[1][:2] == 'JJ' or token[1][:2] == 'NN' or token[1][:2] == 'VB' or token[1][:2] == 'JJ' == 'RB' :\n",
    "                    intermediate_list_token.append(token[0])\n",
    "            list_token.append(intermediate_list_token)\n",
    "        return list_token\n",
    "\n",
    "    filtered_words = sort_words(taggedData)\n",
    "    score_tweet = []\n",
    "        \n",
    "    # Total number of smileys\n",
    "    nb_smileys = 0 \n",
    "    \n",
    "    for tweet in filtered_words :\n",
    "        \n",
    "        modifier = None\n",
    "        negator = None\n",
    "        \n",
    "        score = 0\n",
    "        score_pos = 0\n",
    "        score_neg = 0\n",
    "        \n",
    "        for token in tweet :\n",
    "\n",
    "            try :\n",
    "                if negator in negating :\n",
    "                    if modifier in booster[0] :\n",
    "                        score_pos += swn.senti_synset(wn.synsets(token)[0].name()).neg_score() * 2\n",
    "                        score_neg += swn.senti_synset(wn.synsets(token)[0].name()).pos_score() * 2\n",
    "                    else :\n",
    "                        score_pos += swn.senti_synset(wn.synsets(token)[0].name()).neg_score()\n",
    "                        score_neg += swn.senti_synset(wn.synsets(token)[0].name()).pos_score()\n",
    "                else :\n",
    "                    if modifier in booster[0] :\n",
    "                        score_pos += swn.senti_synset(wn.synsets(token)[0].name()).pos_score() * 2\n",
    "                        score_neg += swn.senti_synset(wn.synsets(token)[0].name()).neg_score() * 2\n",
    "                    else :\n",
    "                        score_pos += swn.senti_synset(wn.synsets(token)[0].name()).pos_score()\n",
    "                        score_neg += swn.senti_synset(wn.synsets(token)[0].name()).neg_score()\n",
    "            \n",
    "            # We now change the except and if the token does not have a synset, we check if it is an emoji\n",
    "            except : \n",
    "                \n",
    "                if token in emo_pos :\n",
    "                    score_pos += 1\n",
    "                    nb_smileys += 1\n",
    "                elif token in emo_neg :\n",
    "                    score_neg += 1\n",
    "                    nb_smileys += 1\n",
    "            \n",
    "            else :\n",
    "                pass\n",
    "            \n",
    "            modifier = token\n",
    "            negator = token\n",
    "            \n",
    "        \n",
    "        if score_pos > score_neg :\n",
    "            score_tweet.append([score_pos, score_neg, 'Positif'])\n",
    "        \n",
    "        elif score_pos == score_neg :\n",
    "            score_tweet.append([score_pos, score_neg, 'Neutre'])\n",
    "        \n",
    "        else : \n",
    "            score_tweet.append([score_pos, score_neg, 'Negatif'])\n",
    "    \n",
    "    print(\"Total number of smileys : \" + str(nb_smileys))\n",
    "    return np.array(score_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:00:08.634036Z",
     "start_time": "2019-06-17T15:00:08.264956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of smileys : 52\n"
     ]
    }
   ],
   "source": [
    "score_v3 = compute_score_v3(taggedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:00:08.792883Z",
     "start_time": "2019-06-17T15:00:08.783784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5742971887550201"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df['sent'], score_v3[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:00:11.349719Z",
     "start_time": "2019-06-17T15:00:11.343929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['sent'] == score_v3[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de détection - V4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En analysant les sorties des algorithmes proposés précédemment, proposez votre propre algorithme d’analyse des opinions dans les tweets et les performances que vous obtenez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:04:33.102095Z",
     "start_time": "2019-06-17T15:04:32.966417Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_score_v4(taggedData, factor) :\n",
    "    \n",
    "    def sort_words(taggedData) :\n",
    "        list_token = []\n",
    "        for tokens in taggedData :\n",
    "            intermediate_list_token = []\n",
    "            for token in tokens :\n",
    "                if token[1][:2] == 'JJ' or token[1][:2] == 'NN' or token[1][:2] == 'VB' or token[1][:2] == 'JJ' == 'RB' :\n",
    "                    intermediate_list_token.append(token[0])\n",
    "            list_token.append(intermediate_list_token)\n",
    "        return list_token\n",
    "\n",
    "    filtered_words = sort_words(taggedData)\n",
    "    score_tweet = []\n",
    "    nb_smileys = 0\n",
    "    \n",
    "    for tweet in filtered_words :\n",
    "        modifier = None\n",
    "        negator = None\n",
    "        \n",
    "        score = 0\n",
    "        score_pos = 0\n",
    "        score_neg = 0\n",
    "        \n",
    "        for token in tweet :\n",
    "            \n",
    "            score_pos_int = 0\n",
    "            score_neg_int = 0\n",
    "            score_obj_int = 0\n",
    "            \n",
    "            tot = len(wn.synsets(token))\n",
    "            \n",
    "            if tot > 0 :\n",
    "                \n",
    "                # We now iterate through all the wordnets of a given token\n",
    "                # And give each definition a decaying weight on the overall score (Factor ^ i where i is position)\n",
    "                for i in range(tot) :\n",
    "                     \n",
    "                    if negator in booster[0] :\n",
    "                        if modifier in booster[0] :\n",
    "                            score_pos_int += swn.senti_synset(wn.synsets(token)[i].name()).neg_score() * 2 * factor ** (i)\n",
    "                            score_neg_int += swn.senti_synset(wn.synsets(token)[i].name()).pos_score() * 2 * factor ** (i)\n",
    "                            score_obj_int += swn.senti_synset(wn.synsets(token)[i].name()).obj_score() * 2 * factor ** (i)\n",
    "                        \n",
    "                        else :\n",
    "                            score_pos_int += swn.senti_synset(wn.synsets(token)[i].name()).neg_score() * factor ** (i)\n",
    "                            score_neg_int += swn.senti_synset(wn.synsets(token)[i].name()).pos_score() * factor ** (i)\n",
    "                            score_obj_int += swn.senti_synset(wn.synsets(token)[i].name()).obj_score() * 2 * factor ** (i)\n",
    "                    else :\n",
    "                        if modifier in booster[0] :\n",
    "                            score_pos_int += swn.senti_synset(wn.synsets(token)[i].name()).pos_score() * 2 * factor ** (i)\n",
    "                            score_neg_int += swn.senti_synset(wn.synsets(token)[i].name()).neg_score() * 2 * factor ** (i)\n",
    "                            score_obj_int += swn.senti_synset(wn.synsets(token)[i].name()).obj_score() * 2 * factor ** (i)\n",
    "                        else :\n",
    "                            score_pos_int += swn.senti_synset(wn.synsets(token)[i].name()).pos_score() * factor ** (i)\n",
    "                            score_neg_int += swn.senti_synset(wn.synsets(token)[i].name()).neg_score() * factor ** (i)\n",
    "                            score_obj_int += swn.senti_synset(wn.synsets(token)[i].name()).obj_score() * 2 * factor ** (i)\n",
    "                \n",
    "            else :\n",
    "                if token in emo_pos :\n",
    "                    score_pos += 1\n",
    "                    nb_smileys += 1\n",
    "                elif token in emo_neg :\n",
    "                    score_neg += 1\n",
    "                    nb_smileys += 1\n",
    "            \n",
    "            # We then normalize the total score to sum to 1 for each token\n",
    "            total = score_pos_int + score_neg_int + score_obj_int\n",
    "            \n",
    "            if total > 0 :\n",
    "                score_pos += score_pos_int / total\n",
    "                score_neg += score_neg_int / total\n",
    "            \n",
    "            modifier = token\n",
    "            negator = token\n",
    "            \n",
    "        # Also change thresholds by allocation more values to Neutral\n",
    "        if score_pos > score_neg + 0.1 :\n",
    "            score_tweet.append([score_pos, score_neg, 'Positif'])\n",
    "        \n",
    "        elif score_pos < score_neg - 0.1 :\n",
    "            score_tweet.append([score_pos, score_neg, 'Negatif'])\n",
    "        \n",
    "        else : \n",
    "            score_tweet.append([score_pos, score_neg, 'Neutre'])\n",
    "    \n",
    "    print(\"Number of smileys : \" + str(nb_smileys))\n",
    "    return np.array(score_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:06:51.745327Z",
     "start_time": "2019-06-17T15:06:48.128621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of smileys : 52\n"
     ]
    }
   ],
   "source": [
    "score_v4 = compute_score_v4(taggedData, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:06:51.755642Z",
     "start_time": "2019-06-17T15:06:51.748639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.642570281124498"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df['sent'], score_v4[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-17T15:05:15.944636Z",
     "start_time": "2019-06-17T15:05:15.938025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['sent'] == score_v4[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat global est amélioré de quasiment 7% avec le seuil rajouté et le facteur de poids."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
