{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPRNNimbdforstudents.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "PBFqSEkKqpCN"
      },
      "cell_type": "markdown",
      "source": [
        "# TP RNN \n",
        "# Using Many-to-One for movie rating predicton\n",
        "\n",
        "For any remark or suggestion, please feel free to contact me at:\n",
        "geoffroy.peeters@telecom-paristech.fr\n",
        "\n",
        "Last edit: 2019/01/15 geoffroy.peeters@telecom-paristech.fr\n",
        "\n",
        "### Objective:\n",
        "We will implement two different networks to perform automatic rating (0 or 1) of a movie given the text of its review.\n",
        "We will use the ```imdb``` (internet movie database) dataset.\n",
        "\n",
        "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QmkCSNaXLqjh"
      },
      "cell_type": "markdown",
      "source": [
        "### Import packages"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T19:07:46.933221Z",
          "start_time": "2019-02-19T19:07:43.343906Z"
        },
        "colab_type": "code",
        "id": "AOqjzDwioJj9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.layers import Dense, Activation, Embedding, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tf.set_random_seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iZDR0JMnPsIj",
        "colab_type": "code",
        "outputId": "7903cd79-19d9-4ff9-aee5-c5d9550fa21f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  device_name = os.environ['COLAB_TPU_ADDR']\n",
        "  TPU_ADDRESS = 'grpc://' + device_name\n",
        "  print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "except KeyError:\n",
        "  print('TPU not found')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found TPU at: grpc://10.51.238.90:8470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "v5Yp4OQVvUtr"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameters of the model\n",
        "\n",
        "-  We only consider the ```top_words``` first words in the word dictionary\n",
        "- We truncate/zerp-pad each sequence a length ```max_review_length````"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T19:12:23.681557Z",
          "start_time": "2019-02-19T19:12:23.677332Z"
        },
        "colab_type": "code",
        "id": "4C_Pv7rYvRkM",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_words = 15000 \n",
        "max_review_length = 250\n",
        "INDEX_FROM = 3\n",
        "epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ZsNcRimyLzgP"
      },
      "cell_type": "markdown",
      "source": [
        "## Import IMDB data"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T19:22:21.736656Z",
          "start_time": "2019-02-19T19:21:10.164131Z"
        },
        "colab_type": "code",
        "id": "5Gfe1ex8oN8Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "133c3793-1e67-4c38-d24f-06de555a572b"
      },
      "cell_type": "code",
      "source": [
        "# Import the IMDB data and only consider the ``top_words``` most used words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=INDEX_FROM)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V0bjN1JuYI2j",
        "colab_type": "code",
        "outputId": "91dc2f4f-5c6c-4a09-926c-356c868ffac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       ...,\n",
              "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 2, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 2]),\n",
              "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 2, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 9685, 6139, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 2, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 2, 8, 1597, 8778, 2, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "3MQOGV-cYKhA",
        "colab_type": "code",
        "outputId": "1474dd4a-658a-45ad-e534-4d88eeb93203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "iSc5LmksOLyr"
      },
      "cell_type": "markdown",
      "source": [
        "## Data content\n",
        "\n",
        "- ```X_train``` and ```X_test``` are numpy arrays of lists. \n",
        "  - each item in a list is the index in the word dictionary. So that a list is the sequence of index of words.\n",
        "\n",
        "- ```y_train``` and ```y_test``` are a numpy arrays of the same dimension as ```X_train``` and ```X_test``` \n",
        "  - they contains the values 0 (bad movie) or 1 (good movie)"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T19:31:03.168199Z",
          "start_time": "2019-02-19T19:31:02.309491Z"
        },
        "colab_type": "code",
        "id": "WouODCPrtiuu",
        "outputId": "afd8c832-7941-42c6-c040-c1f26f1907a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"type(X_train):\", type(X_train))\n",
        "print(\"number of training sequences: X_train.shape:\", X_train.shape)\n",
        "print(\"type(X_train[0]):\",type(X_train[0]))\n",
        "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
        "print(\"length of the second training sequence: len(X_train[0]):\",len(X_train[1]))\n",
        "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
        "len_list = [len(train) for train in X_train]\n",
        "print(\"maximum length of a training sequence:\", max(len_list))\n",
        "\n",
        "plt.hist(len_list, 100);"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type(X_train): <class 'numpy.ndarray'>\n",
            "number of training sequences: X_train.shape: (25000,)\n",
            "type(X_train[0]): <class 'list'>\n",
            "length of the first training sequence: len(X_train[0]): 218\n",
            "length of the second training sequence: len(X_train[0]): 189\n",
            "list of data of the first training sequence: X_train[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "maximum length of a training sequence: 2494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFKCAYAAADScRzUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7g9_n8392hZK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Details of how the reviews are encoded"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T20:16:12.904278Z",
          "start_time": "2019-02-19T20:16:08.057393Z"
        },
        "id": "5cwExMB92hZM",
        "colab_type": "code",
        "outputId": "35409338-5db5-4d02-945a-398579a9c246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "cell_type": "code",
      "source": [
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the '80s that would go directly to video today the film stars champion gymnast kurt thomas as jonathan <UNK> recruited out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a suicidal <UNK> <UNK> by the khan who encourages his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the locals by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more visceral action and faster pace might even be pretty good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T20:17:41.369294Z",
          "start_time": "2019-02-19T20:17:41.363794Z"
        },
        "colab_type": "code",
        "id": "Hfl42LGCugWB",
        "outputId": "03f25817-9c4e-4057-c063-5a6a887a52ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"type(y_train):\", type(y_train))\n",
        "print(\"y_train.shape:\", y_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type(y_train): <class 'numpy.ndarray'>\n",
            "y_train.shape: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T20:17:42.010436Z",
          "start_time": "2019-02-19T20:17:42.004709Z"
        },
        "colab_type": "code",
        "id": "iVw65PNNuobX",
        "outputId": "2de6e594-51d8-497e-f758-440dbe5caa49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_test.shape: (25000,)\n",
            "y_test.shape: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "V18OA7oQNH3c"
      },
      "cell_type": "markdown",
      "source": [
        "## Data processing\n",
        "\n",
        "Sequences (represented as a list of values) in ```X_train``` represent the reviews.\n",
        "They can have different length.\n",
        "To train the network we should modify them so that they all have the same length.\n",
        "We do this by:\n",
        "- truncating the ones that are too long\n",
        "- padding-with-zero them the ones that are too short.\n",
        "\n",
        "This is obtained using ```sequence.pad_sequences``` of keras."
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-19T20:30:09.273755Z",
          "start_time": "2019-02-19T20:30:07.649140Z"
        },
        "colab_type": "code",
        "id": "JhmiHsOGoRwT",
        "outputId": "98757a71-a0f3-4e50-b0d8-0b15634164ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "# truncate and pad input sequences\n",
        "# define sequences\n",
        "\n",
        "# CODE-RNN1-1\n",
        "# --- START CODE HERE\n",
        "X_train = pad_sequences(X_train,max_review_length)\n",
        "X_test = pad_sequences(X_test,max_review_length)\n",
        "# --- END CODE HERE\n",
        "\n",
        "print(\"len(X_train[0]):\", len(X_train[0]))\n",
        "print(\"len(X_train[1]):\", len(X_train[1]))\n",
        "print(\"X_train[0]:\", X_train[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(X_train[0]): 250\n",
            "len(X_train[1]): 250\n",
            "X_train[0]: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     1    14    22    16\n",
            "    43   530   973  1622  1385    65   458  4468    66  3941     4   173\n",
            "    36   256     5    25   100    43   838   112    50   670     2     9\n",
            "    35   480   284     5   150     4   172   112   167     2   336   385\n",
            "    39     4   172  4536  1111    17   546    38    13   447     4   192\n",
            "    50    16     6   147  2025    19    14    22     4  1920  4613   469\n",
            "     4    22    71    87    12    16    43   530    38    76    15    13\n",
            "  1247     4    22    17   515    17    12    16   626    18     2     5\n",
            "    62   386    12     8   316     8   106     5     4  2223  5244    16\n",
            "   480    66  3785    33     4   130    12    16    38   619     5    25\n",
            "   124    51    36   135    48    25  1415    33     6    22    12   215\n",
            "    28    77    52     5    14   407    16    82 10311     8     4   107\n",
            "   117  5952    15   256     4     2     7  3766     5   723    36    71\n",
            "    43   530   476    26   400   317    46     7     4 12118  1029    13\n",
            "   104    88     4   381    15   297    98    32  2071    56    26   141\n",
            "     6   194  7486    18     4   226    22    21   134   476    26   480\n",
            "     5   144    30  5535    18    51    36    28   224    92    25   104\n",
            "     4   226    65    16    38  1334    88    12    16   283     5    16\n",
            "  4472   113   103    32    15    16  5345    19   178    32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YlrDTuk5K65Q"
      },
      "cell_type": "markdown",
      "source": [
        "## First model\n",
        "\n",
        "In the first model, we will simply \n",
        "- learn a word embedding  (```Embedding``` layer in keras) and apply it to each of item of a sequence, \n",
        "  -  in keras, embedding is not a matrix going from one-hot-encoding to embedding, it is a layer that goes from index-in-word-dictionary to embedding\n",
        "  - the embedding goes from ```top_words``` dimensions to  ```embedding_vector_length``` dimensions\n",
        "- average the embedding obtained for each word of a sequence over all words of the sequence (you should use ```K.mean``` and ```Lambda``` from the keras backend)\n",
        "- apply a fully connected (```Dense``` layer in keras) which output activation is a sigmoid ()predicting the 0 or 1 rating)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ufW00TGcs3Jj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B4jYjt2t512i",
        "colab_type": "code",
        "outputId": "a094440f-6169-486c-f4dc-edadeafa3f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# CODE-RNN1-2\n",
        "# --- START CODE HERE\n",
        "def function1(x):\n",
        "    x = K.mean(x, axis=1)\n",
        "    return x\n",
        "  \n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "model.add(Lambda(function1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 250, 32)           480000    \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 480,033\n",
            "Trainable params: 480,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pFXz4AS6tawQ",
        "outputId": "b805e1ea-2d12-4a37-c522-8a924ac7cd75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1856
        }
      },
      "cell_type": "code",
      "source": [
        "# compile and fit the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
        "\n",
        "tpu_model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.51.238.90:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5350107768229588125)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8609402521467275766)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9934158498571369711)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1910827148762559825)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6633747291963205247)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16948386561266680652)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 591918143823418356)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10544793398116837871)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5891433610526078098)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 1614639127036980542)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 17617219670394985050)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(8,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(8, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(8, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f2625dade80> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 2.3065590858459473 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "24768/25000 [============================>.] - ETA: 0s - loss: 0.6517 - acc: 0.7199INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(5,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(5, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(5, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f2625dade80> [<tf.Variable 'tpu_139801807094336/Adam/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f26268ac208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f26268acd30>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f26268966d8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f2626940a90>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f26269a6588>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f2626a2d710>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f2626a11eb8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f2626b7be10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f2626cc3cf8>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 1.8474328517913818 secs\n",
            "24960/25000 [============================>.] - ETA: 0s - loss: 0.6512 - acc: 0.7206INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(8,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(8, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(8, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f2624447e48> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 2.000945806503296 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(5,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(5, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(5, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f2624447e48> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 1.4503090381622314 secs\n",
            "25000/25000 [==============================] - 23s 910us/sample - loss: 0.6511 - acc: 0.7206 - val_loss: 0.5868 - val_acc: 0.7804\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 8s 328us/sample - loss: 0.5058 - acc: 0.8284 - val_loss: 0.4556 - val_acc: 0.8358\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 8s 319us/sample - loss: 0.3930 - acc: 0.8685 - val_loss: 0.3817 - val_acc: 0.8566\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 8s 324us/sample - loss: 0.3276 - acc: 0.8859 - val_loss: 0.3415 - val_acc: 0.8705\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 8s 319us/sample - loss: 0.2868 - acc: 0.8995 - val_loss: 0.3176 - val_acc: 0.8764\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 8s 304us/sample - loss: 0.2577 - acc: 0.9089 - val_loss: 0.3049 - val_acc: 0.8768\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 8s 313us/sample - loss: 0.2348 - acc: 0.9176 - val_loss: 0.2984 - val_acc: 0.8782\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 8s 319us/sample - loss: 0.2163 - acc: 0.9250 - val_loss: 0.2863 - val_acc: 0.8851\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 8s 314us/sample - loss: 0.2001 - acc: 0.9308 - val_loss: 0.2815 - val_acc: 0.8877\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 8s 320us/sample - loss: 0.1857 - acc: 0.9361 - val_loss: 0.2821 - val_acc: 0.8861\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 8s 318us/sample - loss: 0.1735 - acc: 0.9409 - val_loss: 0.2798 - val_acc: 0.8877\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 8s 316us/sample - loss: 0.1619 - acc: 0.9456 - val_loss: 0.2806 - val_acc: 0.8869\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 8s 317us/sample - loss: 0.1514 - acc: 0.9500 - val_loss: 0.2829 - val_acc: 0.8870\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 8s 316us/sample - loss: 0.1418 - acc: 0.9538 - val_loss: 0.2876 - val_acc: 0.8847\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 8s 321us/sample - loss: 0.1325 - acc: 0.9576 - val_loss: 0.2913 - val_acc: 0.8848\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 8s 305us/sample - loss: 0.1246 - acc: 0.9613 - val_loss: 0.2963 - val_acc: 0.8831\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 8s 310us/sample - loss: 0.1169 - acc: 0.9639 - val_loss: 0.3037 - val_acc: 0.8828\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 8s 320us/sample - loss: 0.1094 - acc: 0.9672 - val_loss: 0.3107 - val_acc: 0.8813\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 8s 320us/sample - loss: 0.1026 - acc: 0.9704 - val_loss: 0.3178 - val_acc: 0.8805\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 8s 317us/sample - loss: 0.0961 - acc: 0.9725 - val_loss: 0.3305 - val_acc: 0.8760\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2625dc8630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SBqyzLJRUIsC"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "After only 3 epochs, you should obtain an accuracy around 84% for the test data."
      ]
    },
    {
      "metadata": {
        "id": "L0iHXfdI2hZw",
        "colab_type": "code",
        "outputId": "1c698868-53ec-4b93-8c36-05d85b1674cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = tpu_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(4,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(4, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(4, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f2624447e48> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 1.1577911376953125 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(1,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(1, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(1, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f2624447e48> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 0.9777638912200928 secs\n",
            "Accuracy: 87.60%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_jJwRHMr2hZz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using the trained embedding to find equivalence between words\n",
        "\n",
        "Since the embedding is part of the models, we can look at the trained embedding matrix $E$ and use it to get the most similar words (according to the trained matrix $E$) in the dictionary.\n",
        "Use the weights of the ```Embedding``` layer to find the most similar words to ```amazing```. We will use an Euclidean distance for that.\n",
        "- Retrieve the weights of the ```Embedding layer```\n",
        "- Get the position of ```amazing``` in the dictionary\n",
        "- Get the word-embedding of ```amazing```\n",
        "- Find (using Euclidean distance), the closest embedded-words to ```amazing```"
      ]
    },
    {
      "metadata": {
        "id": "xrElmfg_Ai4O",
        "colab_type": "code",
        "outputId": "33a15944-b65e-47e9-f903-d7c8f65a6948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# CODE-RNN1-3\n",
        "# --- START CODE HERE\n",
        "weights = tpu_model.layers[1].get_weights()[0]\n",
        "\n",
        "def closest_node(node, nodes, id_to_word):\n",
        "    nodes = np.asarray(nodes)\n",
        "    dist_2 = np.sum((nodes - node)**2, axis=1)\n",
        "    pos = np.argsort(dist_2)[:5]\n",
        "    list_words = []\n",
        "    for p in pos :\n",
        "      list_words.append(id_to_word.get(p))\n",
        "    return pos, list_words\n",
        " \n",
        "closest_word = closest_node(weights[word_to_id['amazing']], weights, id_to_word)\n",
        "closest_word\n",
        "# --- END CODE HERE"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  480,  6324,  1119,  3678, 10165]),\n",
              " ['amazing', 'colleagues', '15', 'mate', '1952'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zK9e5Eo1Ks2a"
      },
      "cell_type": "markdown",
      "source": [
        "## Second model\n",
        "\n",
        "In the second model, we will replace\n",
        "- average the obtained embedding over the sequence (use ```K.mean``` and ```Lambda```from keras backend)\n",
        "- by a RNN layer (more precisely an ```LSTM```) in a Many-To-One configuration with $n_a=100$\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rwoXuOqqVDOy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7dl-CSMKoViX",
        "outputId": "60b186dd-5a3c-4b71-e831-3aa1adc21ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "\n",
        "# CODE-RNN1-4\n",
        "# --- START CODE HERE\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "# --- END CODE HERE"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 250, 32)           480000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 533,301\n",
            "Trainable params: 533,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-bp7PzX7oXtB",
        "outputId": "729a8c42-8c94-4861-85be-4b0558a1584c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1635
        }
      },
      "cell_type": "code",
      "source": [
        "# compile and fit the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "    model,\n",
        "    strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "    tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)))\n",
        "\n",
        "tpu_model.fit(X_train, y_train, epochs=epochs, batch_size=256, validation_data=(X_test, y_test))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.51.238.90:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 5350107768229588125)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 8609402521467275766)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 9934158498571369711)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 1910827148762559825)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 6633747291963205247)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 16948386561266680652)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 591918143823418356)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 10544793398116837871)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5891433610526078098)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 1614639127036980542)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 17617219670394985050)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/20\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(32, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(32, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f262e10eeb8> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 3.714871644973755 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "24320/25000 [============================>.] - ETA: 0s - loss: 0.6116 - acc: 0.6778INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(21,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(21, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(21, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f262e10eeb8> [<tf.Variable 'tpu_139801846083712/Adam/iterations:0' shape=() dtype=int64>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261ce1a198>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261ce1acc0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261ce34668>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cdf9208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cd43208>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cd69da0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cd354e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cca7e10>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cc1e748>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cbe64e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cbb22e8>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cb57ef0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261cb27c88>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261ca97c18>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261ca0dbe0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261c9d44e0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261c9a17f0>, <tensorflow.contrib.tpu.python.tpu.keras_tpu_variables.ReplicatedVariable object at 0x7f261c96cda0>]\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.0059814453125 secs\n",
            "24832/25000 [============================>.] - ETA: 0s - loss: 0.6117 - acc: 0.6782INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(32,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(32, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(32, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f261ad4de80> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 6.73009991645813 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(21,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(21, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(21, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f261ad4de80> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.50934100151062 secs\n",
            "25000/25000 [==============================] - 52s 2ms/sample - loss: 0.6116 - acc: 0.6784 - val_loss: 0.5974 - val_acc: 0.6994\n",
            "Epoch 2/20\n",
            "25000/25000 [==============================] - 6s 222us/sample - loss: 0.3798 - acc: 0.8399 - val_loss: 0.7973 - val_acc: 0.7720\n",
            "Epoch 3/20\n",
            "25000/25000 [==============================] - 6s 221us/sample - loss: 0.2832 - acc: 0.8906 - val_loss: 2.1260 - val_acc: 0.5747\n",
            "Epoch 4/20\n",
            "25000/25000 [==============================] - 6s 221us/sample - loss: 0.3453 - acc: 0.8642 - val_loss: 1.1345 - val_acc: 0.6992\n",
            "Epoch 5/20\n",
            "25000/25000 [==============================] - 5s 219us/sample - loss: 0.2582 - acc: 0.9014 - val_loss: 0.5257 - val_acc: 0.7854\n",
            "Epoch 6/20\n",
            "25000/25000 [==============================] - 6s 221us/sample - loss: 0.1712 - acc: 0.9378 - val_loss: 0.4660 - val_acc: 0.8190\n",
            "Epoch 7/20\n",
            "25000/25000 [==============================] - 6s 223us/sample - loss: 0.1290 - acc: 0.9578 - val_loss: 0.4408 - val_acc: 0.8400\n",
            "Epoch 8/20\n",
            "25000/25000 [==============================] - 6s 220us/sample - loss: 0.0989 - acc: 0.9697 - val_loss: 0.5474 - val_acc: 0.7905\n",
            "Epoch 9/20\n",
            "25000/25000 [==============================] - 6s 222us/sample - loss: 0.0944 - acc: 0.9712 - val_loss: 0.4587 - val_acc: 0.8108\n",
            "Epoch 10/20\n",
            "25000/25000 [==============================] - 6s 223us/sample - loss: 0.0783 - acc: 0.9766 - val_loss: 0.5156 - val_acc: 0.8042\n",
            "Epoch 11/20\n",
            "25000/25000 [==============================] - 5s 218us/sample - loss: 0.0736 - acc: 0.9779 - val_loss: 0.5255 - val_acc: 0.8242\n",
            "Epoch 12/20\n",
            "25000/25000 [==============================] - 5s 216us/sample - loss: 0.0550 - acc: 0.9843 - val_loss: 0.5900 - val_acc: 0.8127\n",
            "Epoch 13/20\n",
            "25000/25000 [==============================] - 6s 221us/sample - loss: 0.0512 - acc: 0.9856 - val_loss: 0.5245 - val_acc: 0.8275\n",
            "Epoch 14/20\n",
            "25000/25000 [==============================] - 6s 227us/sample - loss: 0.0452 - acc: 0.9867 - val_loss: 0.5569 - val_acc: 0.8378\n",
            "Epoch 15/20\n",
            "25000/25000 [==============================] - 6s 227us/sample - loss: 0.0323 - acc: 0.9917 - val_loss: 0.6868 - val_acc: 0.8261\n",
            "Epoch 16/20\n",
            "25000/25000 [==============================] - 6s 222us/sample - loss: 0.0362 - acc: 0.9894 - val_loss: 0.6113 - val_acc: 0.8341\n",
            "Epoch 17/20\n",
            "25000/25000 [==============================] - 6s 220us/sample - loss: 0.0470 - acc: 0.9848 - val_loss: 0.6126 - val_acc: 0.8362\n",
            "Epoch 18/20\n",
            "25000/25000 [==============================] - 6s 222us/sample - loss: 0.0304 - acc: 0.9922 - val_loss: 0.6937 - val_acc: 0.8271\n",
            "Epoch 19/20\n",
            "25000/25000 [==============================] - 6s 220us/sample - loss: 0.0278 - acc: 0.9928 - val_loss: 0.7314 - val_acc: 0.8407\n",
            "Epoch 20/20\n",
            "25000/25000 [==============================] - 5s 219us/sample - loss: 0.0168 - acc: 0.9963 - val_loss: 0.7823 - val_acc: 0.8352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f262e173358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "F1LN_fjMWBHJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "After only 3 epochs, you should obtain an accuracy around 88% for the test data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RlMEKRbzoavm",
        "outputId": "268ec745-e36c-43dd-8ab1-2b77e1857e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "cell_type": "code",
      "source": [
        "# Final evaluation of the model\n",
        "scores = tpu_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(4,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(4, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(4, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f261ad4de80> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.3865110874176025 secs\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(1,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(1, 250), dtype=tf.float32, name='embedding_input_10'), TensorSpec(shape=(1, 1), dtype=tf.float32, name='dense_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Remapping placeholder for embedding_input\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f261ad4de80> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 5.7634501457214355 secs\n",
            "Accuracy: 83.52%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}